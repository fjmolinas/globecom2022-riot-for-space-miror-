\section{Cubedate Evaluation and Discussion}
\label{sec:evaluation}

%The analysis of the implementation was two-fold, first we evaluate how the SUIT based architecture fitted the CubeSat and CubeSat hosted payload use case. Second, we discuss possible modifications to the workflow, architecture and protocols.

%\subsection{Evaluation}

In the following, code measurements where generated compiling with ARM GCC 10.2.1,
optimized for code size. As code base, we used RIOT release 2022.01 and SUIT configured with ed25519 digital signatures provided by the C25519 crypto library as backend (which has small footprint as shown in prior work~\cite{zandberg2019secure}).

\subsubsection{Memory Footprint Overhead}

To evaluate the RAM and Flash footprint of our Cubedate implementation, we apply it to the ThingSat use-case, compiled for the hosted payload hardware described in \autoref{sec:thingsat-hw} (based on a Cortex-M microcontroller).

In \autoref{tab:footprint} we compare the RAM and Flash memory requirements for a ThingSat firmware without/with Cubedate-compliant secure software updates.
We observe that Cubedate requires a memory budget of $\sim$4kB of RAM and $\sim$19kB of Flash, which represents roughly a 10\% increase in the total RAM and Flash memory budget for ThingSat.

\subsubsection{Network Transfer Overhead}
On the UHF/VHF link at 1kb/s, the additional network transfer time induced by the Cubedate firmware size overhead (19kB) is roughly 15 seconds. This overhead is reasonable, but not negligible keeping in mind that a connection to the CubeSat is segmented in time windows of $\sim$ 300 seconds.

Next, in \autoref{tab:footprint} we look in more details at the metadata (SUIT manifest) used to secure Cubedate software updates for ThingSat.
As we can see, the metadata including all CBOR/COSE formatting, digests (SHA256 hashes) and authentication data (ed25519 signature) amounts to $\sim$330B. The metadata overhead thus incurs negligible overhead (+0,15\%) in case of a ThingSat firmware update (of size 200kB on average, recall \autoref{sec:thingsat-update-req}). However, in case of a smaller software update such as updating a mission scenario (average size 700B) the overhead of adding Cubedate metadata is significant (almost +50\%).
Nevertheless, on the UHF/VHF link at 1kb/s, this overhead remains negligible in terms of additional network transfer time.

\iffalse

architecture we evaluate them
in two configurations, furthermore we categorize the different components to
distinguish the SUIT endured overhead

\begin{itemize}
    \item \textbf{ThingSat} refers to the ThingSat payload application with no
    software updates support
    \item \textbf{Cubedate} implementation of the Cubedate architecture on the
    ThingSat payload
    \item CAN (Controller Area Network) stack as well as low level interface
    \item Crypto includes all cryptographic algorithms such as digest algorithm, digital
    signature, ECC and bignum, as well as pseudo-random numbers generator
    \item CoAP protocol library (CoAP endpoint stack excluded)
    \item CSP (Cube Sat Protocol) network stack
    \item LoRa GW includes the sx1302 driver as well as the gateway code
    \item SUIT englobe all components enabling retrieval and installation of suit data
    (fw or other), this include e.g. the CoAP endpoint stack.
    \item Firmware: application specific code related to the CubeSat Payload excluding
    the LoRa gateway
\end{itemize}

The flash memory footprints (total and broken down per component) are shown in
\autoref{tab:footprint}. It can be seen that the overhead of Cubedate for ThingSat
is of \~4KB of RAM and \~19KB of flash.
\fi


\begin{table}[ht]
\begin{adjustbox}{width=0.8\columnwidth,center}
    \centering
    \includestandalone[width=1\columnwidth]{Figures/texfigs/memory_footprint_table}
\end{adjustbox}
\caption{Cubedate implementation: memory footprint in B.}
\label{tab:footprint}
\end{table}

\subsubsection{Standarized Network Stack Usage}

CSP was designed to give CubeSat sub-systems developers the same features as an
TCP/IP stack without the overhead of the IP header, allowing to run on constrained
systems with under 4KB of RAM. Unless running on this kind of very constrained
devices using CSP can be contested. A minimal CoAP server example running on
LibCSP or RIOTs default IPv6/UDP network stack (GNRC) yields similar numbers of RAM/Flash
usage: 10KB\/30KB (CSP) vs 8KB/31KB (GNRC). Optimal compression of an Ipv6 header
can shrink the size from 48B to 4B, comparable to CSP2.0 3B header.
Furthermore  6loCAN\cite{wachter20206locan01}, SCHC\cite{rfc8724}, allow to
optimize IPv6 for CAN-BUS or LoRa network. What is gained in simplicity is eventually
lost by using a not standard network stack, i.e. using standard application
layer protocols.

\begin{table}[ht]

\begin{adjustbox}{width=0.5\columnwidth,center}
    \centering
    \includestandalone[width=1\columnwidth]{Figures/texfigs/manifest_overhead_table_simplified}
\end{adjustbox}
\caption{Cubedate implementation: SUIT metadata overhead.}
\label{tab:manifest-overhead}
\end{table}

\subsubsection{SUIT Authentication}

SUIT relies on COSE for authentication, with digital signatures being the most
used method. A viable alternative for CubeSat maybe to instead use a tagged Message
Authentication Code (MAC). This will reduce the size of \textit{Authentication} block
\ref*{tab:manifest-overhead} from 64B to 16B or 8B if using HAMC-256/128
or HMAC-256/64. This would also save Flash since as seen in~\cite{zandberg2019secure}
the ed25519 library accounts for roughly 75\% of the crypto flash budget, while HMAC-256
re-uses sha256 code, and adds an overhead of ~1Kbyte. The requirement for a shared secret
might be an acceptable compromise since attackers gaining physical access to the
in-orbit device is unlikely.

\subsubsection{SUIT Manifest Overhead}

In space communication transferring data is costly, reducing the Manifest
overhead~\ref{tab:manifest-overhead} for mission files transfers is highly
desirable. This can be done by using \texttt{COSE\_MAC0\_tagged} instead of
\texttt{COSE\_SGN0\_tagged} (-56B) and integrating the payload into the
manifest (-64B from the URI), reducing the manifest size to 162B,
effectively reducing the overhead to \~23\%.

